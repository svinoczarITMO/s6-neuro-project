{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf4b2b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: IMG_5264.wav\n",
      "Found 12 chunks\n",
      "Chunk 1: \n",
      "Chunk 2: я вас любил\n",
      "Chunk 3: любовь еще быть может\n",
      "Chunk 4: в душе моей угаслы не совсем\n",
      "Chunk 5: но пусть она вас больше не тревожит\n",
      "Chunk 6: я не хочу печалить вас ничем\n",
      "Chunk 7: я вас любил безмогвно безнадежно\n",
      "Chunk 8: то робостью то ревностью томим\n",
      "Chunk 9: я вас любил так искренне так нежно\n",
      "Chunk 10: \n",
      "Chunk 11: когдай вам бог любимый быть другим\n",
      "Chunk 12: \n",
      "\n",
      "Final Result:\n",
      " я вас любил любовь еще быть может в душе моей угаслы не совсем но пусть она вас больше не тревожит я не хочу печалить вас ничем я вас любил безмогвно безнадежно то робостью то ревностью томим я вас любил так искренне так нежно  когдай вам бог любимый быть другим \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# Загрузка модели\n",
    "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "\n",
    "def split_audio(audio_path, min_silence_len=500, silence_thresh=-40, keep_silence=200):\n",
    "    \"\"\"Разбиваем аудио на фрагменты по тишине\"\"\"\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    chunks = split_on_silence(\n",
    "        audio,\n",
    "        min_silence_len=min_silence_len,\n",
    "        silence_thresh=silence_thresh,\n",
    "        keep_silence=keep_silence\n",
    "    )\n",
    "    return chunks\n",
    "\n",
    "def process_chunk(chunk, chunk_idx):\n",
    "    \"\"\"Обработка одного аудиофрагмента\"\"\"\n",
    "    # Экспортируем фрагмент во временный файл\n",
    "    chunk.export(f\"temp_chunk_{chunk_idx}.wav\", format=\"wav\")\n",
    "    \n",
    "    # Загружаем как массив numpy\n",
    "    speech_array, sr = librosa.load(f\"temp_chunk_{chunk_idx}.wav\", sr=16_000)\n",
    "    \n",
    "    # Обработка через модель\n",
    "    inputs = processor(speech_array, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "    \n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    return processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "def analyze_audio(audio_path):\n",
    "    \"\"\"Основная функция анализа\"\"\"\n",
    "    print(f\"\\nAnalyzing: {audio_path}\")\n",
    "    \n",
    "    # Разбиваем на фрагменты\n",
    "    chunks = split_audio(audio_path)\n",
    "    print(f\"Found {len(chunks)} chunks\")\n",
    "    \n",
    "    # Обрабатываем каждый фрагмент\n",
    "    full_text = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            text = process_chunk(chunk, i)\n",
    "            print(f\"Chunk {i+1}: {text}\")\n",
    "            full_text.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i+1}: {str(e)}\")\n",
    "    \n",
    "    # Объединяем результаты\n",
    "    return \" \".join(full_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_paths = [\"IMG_5264.wav\"]  # Ваши аудиофайлы\n",
    "    \n",
    "    for audio_path in audio_paths:\n",
    "        try:\n",
    "            result = analyze_audio(audio_path)\n",
    "            print(\"\\nFinal Result:\")\n",
    "            print(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {audio_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LANG_ID = \"ru\"\n",
    "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"\n",
    "SAMPLES = 5\n",
    "\n",
    "test_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to read the audio files as arrays\n",
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n",
    "    batch[\"speech\"] = speech_array\n",
    "    batch[\"sentence\"] = batch[\"sentence\"].upper()\n",
    "    return batch\n",
    "\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
    "inputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "predicted_sentences = processor.batch_decode(predicted_ids)\n",
    "\n",
    "for i, predicted_sentence in enumerate(predicted_sentences):\n",
    "    print(\"-\" * 100)\n",
    "    print(\"Reference:\", test_dataset[i][\"sentence\"])\n",
    "    print(\"Prediction:\", predicted_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
